"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""
import time
import sys
from transformers import pipeline
import torch


def test_model_performance(model_name: str = "facebook/bart-large-cnn"):
    """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    import torch
    
    print("=" * 60)
    print("–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU/MPS
    has_cuda = torch.cuda.is_available()
    has_mps = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
    has_gpu = has_cuda or has_mps
    
    if has_cuda:
        device_name = "CUDA GPU"
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"\n‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω CUDA GPU: {gpu_name}")
        print(f"   –ü–∞–º—è—Ç—å GPU: {gpu_memory:.1f} GB")
    elif has_mps:
        device_name = "MPS (Apple Silicon)"
        print(f"\n‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω MPS (Metal Performance Shaders)")
        print(f"   Apple Silicon GPU –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è")
    else:
        device_name = "CPU"
        print(f"\n‚ö†Ô∏è  GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU")
    
    print(f"\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
    print("   –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 1-3 –º–∏–Ω—É—Ç—ã –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ...")
    
    try:
        from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
        import torch
        
        device = "mps" if has_gpu and torch.backends.mps.is_available() else "cpu"
        print(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: {device}")
        
        print("   –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print("   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        model.to(device)
        model.eval()
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        summarizer = {
            'tokenizer': tokenizer,
            'model': model,
            'device': device
        }
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    test_cases = [
        ("–ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç", 100, "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. " * 10),
        ("–°—Ä–µ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç", 500, "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. " * 50),
        ("–î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç", 1000, "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. " * 100),
        ("–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç", 2000, "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. " * 200),
    ]
    
    print("\n" + "=" * 60)
    print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
    print("=" * 60)
    
    results = []
    
    for name, word_count, text in test_cases:
        print(f"\nüìù –¢–µ—Å—Ç: {name} (~{word_count} —Å–ª–æ–≤)")
        
        start_time = time.time()
        try:
            import torch
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = summarizer['tokenizer'](
                text,
                max_length=1024,
                truncation=True,
                return_tensors="pt"
            ).to(summarizer['device'])
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            with torch.no_grad():
                outputs = summarizer['model'].generate(
                    inputs["input_ids"],
                    max_length=100,
                    min_length=50,
                    num_beams=4,
                    early_stopping=True,
                    do_sample=False
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            summary = summarizer['tokenizer'].decode(outputs[0], skip_special_tokens=True)
            elapsed = time.time() - start_time
            print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {elapsed:.2f} —Å–µ–∫—É–Ω–¥")
            print(f"   üìä –°–∫–æ—Ä–æ—Å—Ç—å: {word_count/elapsed:.1f} —Å–ª–æ–≤/—Å–µ–∫")
            print(f"   ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {summary[:80]}...")
            
            results.append({
                'name': name,
                'words': word_count,
                'time': elapsed,
                'speed': word_count / elapsed
            })
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
            results.append({
                'name': name,
                'words': word_count,
                'time': None,
                'speed': None
            })
    
    # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
    print("\n" + "=" * 60)
    print("–ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞:")
    print("=" * 60)
    
    avg_speed = sum(r['speed'] for r in results if r['speed']) / len([r for r in results if r['speed']])
    
    print(f"\nüìà –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {avg_speed:.1f} —Å–ª–æ–≤/—Å–µ–∫—É–Ω–¥—É")
    
    # –û—Ü–µ–Ω–∫–∞ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    typical_words = 5000  # –¢–∏–ø–∏—á–Ω—ã–π –æ–±—ä–µ–º –¥–ª—è —Å–≤–æ–¥–∫–∏
    estimated_time = typical_words / avg_speed if avg_speed > 0 else None
    
    if estimated_time:
        print(f"\n‚è±Ô∏è  –û—Ü–µ–Ω–∫–∞ –¥–ª—è —Ç–∏–ø–∏—á–Ω–æ–π —Å–≤–æ–¥–∫–∏ (~{typical_words} —Å–ª–æ–≤):")
        print(f"   –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: ~{estimated_time:.0f} —Å–µ–∫—É–Ω–¥ ({estimated_time/60:.1f} –º–∏–Ω—É—Ç)")
        
        if estimated_time < 30:
            print("   ‚úÖ –û—Ç–ª–∏—á–Ω–æ! –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω—ã—Ö —Å–≤–æ–¥–æ–∫")
        elif estimated_time < 60:
            print("   ‚úÖ –•–æ—Ä–æ—à–æ! –ü—Ä–∏–µ–º–ª–µ–º–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å")
        elif estimated_time < 120:
            print("   ‚ö†Ô∏è  –ú–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API")
        else:
            print("   ‚ùå –°–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å API")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\n" + "=" * 60)
    print("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    print("=" * 60)
    
    if has_gpu:
        print("\n‚úÖ –£ –≤–∞—Å –µ—Å—Ç—å GPU - –æ—Ç–ª–∏—á–Ω–æ!")
        print("   –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±—ã—Å—Ç—Ä–æ")
    else:
        print("\n‚ö†Ô∏è  GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω")
        print("   –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ CPU (–º–µ–¥–ª–µ–Ω–Ω–µ–µ)")
        print("   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏")
    
    print("\nüí° –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –±–æ—Ç–µ:")
    print(f"   1. –î–æ–±–∞–≤—å—Ç–µ –≤ .env: LOCAL_MODEL_PATH={model_name}")
    print("   2. –û—Å—Ç–∞–≤—å—Ç–µ OPENAI_API_KEY –ø—É—Å—Ç—ã–º")
    print("   3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –±–æ—Ç–∞: python main.py")
    
    return True


if __name__ == "__main__":
    model = sys.argv[1] if len(sys.argv) > 1 else "facebook/bart-large-cnn"
    test_model_performance(model)
